{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    loss=y-np.dot(tx,w)\n",
    "    return np.mean(np.power(loss,2))/2\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2694.4833658870843"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.array([1,2])\n",
    "print(w.shape)\n",
    "compute_loss(y,tx,w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    for i_w0 in range(len(grid_w0)):\n",
    "        for k_w1 in range(len(grid_w1)):\n",
    "            losses[i_w0,k_w1]=compute_loss(y,tx,np.array([grid_w0[i_w0],grid_w1[k_w1]]))\n",
    "    return losses\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=66.92119404701603, w0*=72.72727272727272, w1*=10.606060606060595, execution time=0.181 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "a=np.array([72.72,13.63])\n",
    "compute_gradient(y,tx,a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    return -np.dot(tx.T,y-np.dot(tx,w))/len(y)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters + 1 containing the model parameters as numpy arrays of shape (2, ),\n",
    "            for each iteration of GD (as well as the final weights)\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        gradient=compute_gradient(y,tx,w)\n",
    "        loss=compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        # ***************************************************\n",
    "       # raise NotImplementedError\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=378.0473881987588, w0=97.32939220021052, w1=10.347971243498908\n",
      "GD iter. 1/49: loss=309.1417031360723, w0=94.92584518039999, w1=10.661145362647925\n",
      "GD iter. 2/49: loss=253.32809823529618, w0=92.7626528625705, w1=10.94300206988204\n",
      "GD iter. 3/49: loss=208.1190782656674, w0=90.81577977652397, w1=11.196673106392742\n",
      "GD iter. 4/49: loss=171.4997720902681, w0=89.06359399908209, w1=11.424977039252376\n",
      "GD iter. 5/49: loss=141.8381340881947, w0=87.4866267993844, w1=11.630450578826045\n",
      "GD iter. 6/49: loss=117.81220730651526, w0=86.06735631965647, w1=11.815376764442346\n",
      "GD iter. 7/49: loss=98.35120661335485, w0=84.79001288790134, w1=11.981810331497018\n",
      "GD iter. 8/49: loss=82.58779605189501, w0=83.64040379932173, w1=12.131600541846222\n",
      "GD iter. 9/49: loss=69.81943349711261, w0=82.60575561960007, w1=12.266411731160506\n",
      "GD iter. 10/49: loss=59.47705982773877, w0=81.67457225785058, w1=12.387741801543362\n",
      "GD iter. 11/49: loss=51.09973715554598, w0=80.83650723227605, w1=12.496938864887932\n",
      "GD iter. 12/49: loss=44.31410579106985, w0=80.08224870925896, w1=12.595216221898044\n",
      "GD iter. 13/49: loss=38.817744385844165, w0=79.40341603854358, w1=12.683665843207145\n",
      "GD iter. 14/49: loss=34.36569164761134, w0=78.79246663489974, w1=12.763270502385335\n",
      "GD iter. 15/49: loss=30.75952892964275, w0=78.24261217162028, w1=12.834914695645708\n",
      "GD iter. 16/49: loss=27.83853712808819, w0=77.74774315466877, w1=12.899394469580042\n",
      "GD iter. 17/49: loss=25.47253376882901, w0=77.3023610394124, w1=12.957426266120942\n",
      "GD iter. 18/49: loss=23.556071047829057, w0=76.90151713568169, w1=13.009654883007753\n",
      "GD iter. 19/49: loss=22.00373624381914, w0=76.54075762232404, w1=13.056660638205884\n",
      "GD iter. 20/49: loss=20.746345052571087, w0=76.21607406030215, w1=13.098965817884201\n",
      "GD iter. 21/49: loss=19.727858187660157, w0=75.92385885448245, w1=13.137040479594686\n",
      "GD iter. 22/49: loss=18.902883827082302, w0=75.66086516924472, w1=13.171307675134123\n",
      "GD iter. 23/49: loss=18.234654595014234, w0=75.42417085253076, w1=13.202148151119616\n",
      "GD iter. 24/49: loss=17.69338891703912, w0=75.2111459674882, w1=13.22990457950656\n",
      "GD iter. 25/49: loss=17.254963717879274, w0=75.01942357094991, w1=13.25488536505481\n",
      "GD iter. 26/49: loss=16.899839306559805, w0=74.84687341406544, w1=13.277368072048233\n",
      "GD iter. 27/49: loss=16.61218853339103, w0=74.69157827286942, w1=13.297602508342315\n",
      "GD iter. 28/49: loss=16.37919140712433, w0=74.55181264579299, w1=13.31581350100699\n",
      "GD iter. 29/49: loss=16.190463734848286, w0=74.42602358142422, w1=13.332203394405195\n",
      "GD iter. 30/49: loss=16.037594320304702, w0=74.31281342349232, w1=13.34695429846358\n",
      "GD iter. 31/49: loss=15.9137700945244, w0=74.2109242813536, w1=13.360230112116126\n",
      "GD iter. 32/49: loss=15.813472471642342, w0=74.11922405342875, w1=13.372178344403418\n",
      "GD iter. 33/49: loss=15.732231397107878, w0=74.0366938482964, w1=13.38293175346198\n",
      "GD iter. 34/49: loss=15.666426126734967, w0=73.96241666367727, w1=13.392609821614688\n",
      "GD iter. 35/49: loss=15.613123857732912, w0=73.89556719752007, w1=13.401320082952124\n",
      "GD iter. 36/49: loss=15.569949019841244, w0=73.83540267797858, w1=13.409159318155815\n",
      "GD iter. 37/49: loss=15.534977401148993, w0=73.78125461039124, w1=13.41621462983914\n",
      "GD iter. 38/49: loss=15.506650390008272, w0=73.73252134956263, w1=13.42256441035413\n",
      "GD iter. 39/49: loss=15.483705510984285, w0=73.68866141481689, w1=13.428279212817623\n",
      "GD iter. 40/49: loss=15.465120158974855, w0=73.64918747354572, w1=13.433422535034765\n",
      "GD iter. 41/49: loss=15.450066023847222, w0=73.61366092640166, w1=13.438051525030193\n",
      "GD iter. 42/49: loss=15.437872174393833, w0=73.58168703397202, w1=13.44221761602608\n",
      "GD iter. 43/49: loss=15.427995156336591, w0=73.55291053078534, w1=13.445967097922376\n",
      "GD iter. 44/49: loss=15.419994771710227, w0=73.52701167791733, w1=13.449341631629043\n",
      "GD iter. 45/49: loss=15.41351446016287, w0=73.50370271033611, w1=13.452378711965043\n",
      "GD iter. 46/49: loss=15.40826540780951, w0=73.48272463951302, w1=13.455112084267444\n",
      "GD iter. 47/49: loss=15.404013675403291, w0=73.46384437577224, w1=13.457572119339606\n",
      "GD iter. 48/49: loss=15.40056977215425, w0=73.44685213840553, w1=13.45978615090455\n",
      "GD iter. 49/49: loss=15.397780210522527, w0=73.4315591247755, w1=13.461778779313\n",
      "GD: execution time=0.036 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([100, 10])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4067c5906d9474b88107e569cc4558c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    return -np.dot(tx.T,y-np.dot(tx,w))/len(y)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        for batch_y, batch_tx in batch_iter(y,tx,batch_size):\n",
    "            loss=compute_loss(batch_y,batch_tx,w)\n",
    "            losses.append(loss)\n",
    "            ws.append(w)\n",
    "            w=w-gamma*compute_stoch_gradient(batch_y,batch_tx,w)\n",
    "        # ***************************************************\n",
    "        \n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=4546.58607173972, w0=9.53581257338851, w1=13.003280108129093\n",
      "SGD iter. 1/49: loss=1602.2961305103938, w0=15.196724391303349, w1=4.699394793662737\n",
      "SGD iter. 2/49: loss=1300.4380944425047, w0=20.29660300641274, w1=2.5462941847935303\n",
      "SGD iter. 3/49: loss=729.4753323429369, w0=24.116224276494208, w1=0.8322319254787816\n",
      "SGD iter. 4/49: loss=977.4740159162787, w0=28.53770370292215, w1=-2.5916403753358006\n",
      "SGD iter. 5/49: loss=1688.6588457934029, w0=34.34917313120516, w1=-2.1881341123325533\n",
      "SGD iter. 6/49: loss=170.7039542007869, w0=36.196895806288126, w1=-5.000543741481332\n",
      "SGD iter. 7/49: loss=768.8940992514995, w0=40.11836007171628, w1=-4.407552365789271\n",
      "SGD iter. 8/49: loss=2403.0292510435875, w0=47.05093427044447, w1=7.513310568756718\n",
      "SGD iter. 9/49: loss=671.3936274267813, w0=50.71534043632396, w1=6.8561645110091165\n",
      "SGD iter. 10/49: loss=217.06721142470386, w0=52.798929702108865, w1=5.040236601534615\n",
      "SGD iter. 11/49: loss=161.54681756181722, w0=54.596410258675846, w1=3.3328320793850823\n",
      "SGD iter. 12/49: loss=147.69518435736828, w0=56.31510269397291, w1=3.478066800168549\n",
      "SGD iter. 13/49: loss=294.03552786280073, w0=58.74012033458009, w1=5.641798214617431\n",
      "SGD iter. 14/49: loss=297.49568304182037, w0=61.179364820242895, w1=7.019711512733386\n",
      "SGD iter. 15/49: loss=69.06047015679854, w0=62.35461347611788, w1=4.397450085941825\n",
      "SGD iter. 16/49: loss=30.76500070596897, w0=63.13902409187941, w1=4.592394806623179\n",
      "SGD iter. 17/49: loss=135.012664907551, w0=64.78226883876012, w1=6.020067710647778\n",
      "SGD iter. 18/49: loss=126.03935658253218, w0=66.36996752920822, w1=7.227215206561722\n",
      "SGD iter. 19/49: loss=17.14738255176347, w0=65.78435020446854, w1=7.525820089647223\n",
      "SGD iter. 20/49: loss=175.45044319409803, w0=67.6575850706511, w1=9.015183508669118\n",
      "SGD iter. 21/49: loss=7.362945575782097, w0=68.04132835921213, w1=8.622817687145442\n",
      "SGD iter. 22/49: loss=2.161095004205008, w0=68.24922713285955, w1=8.40759003237926\n",
      "SGD iter. 23/49: loss=96.38762356148224, w0=69.63766239286616, w1=11.174488116842928\n",
      "SGD iter. 24/49: loss=0.4432931957587258, w0=69.54350367624802, w1=11.234345600846265\n",
      "SGD iter. 25/49: loss=44.05487799680331, w0=70.48217164733049, w1=9.57530510459625\n",
      "SGD iter. 26/49: loss=44.86895629045853, w0=71.42947261629926, w1=10.320703728723002\n",
      "SGD iter. 27/49: loss=21.52338577140626, w0=70.77337223112325, w1=11.069002237378175\n",
      "SGD iter. 28/49: loss=3.3694509088906353, w0=70.51377828246918, w1=10.974045793010667\n",
      "SGD iter. 29/49: loss=3.967573451563713, w0=70.23208435454158, w1=10.98217633851735\n",
      "SGD iter. 30/49: loss=8.46705799456889, w0=69.82057352857053, w1=10.375565328574915\n",
      "SGD iter. 31/49: loss=27.59782273366076, w0=70.56351124781301, w1=10.849032514987524\n",
      "SGD iter. 32/49: loss=19.464405103415373, w0=69.93958168282326, w1=11.123213359101179\n",
      "SGD iter. 33/49: loss=23.351094706817992, w0=70.62297168431316, w1=11.558731347820132\n",
      "SGD iter. 34/49: loss=8.005702208474094, w0=71.02311421413157, w1=11.546254047054424\n",
      "SGD iter. 35/49: loss=17.620315348696405, w0=71.6167524094778, w1=10.72180350924428\n",
      "SGD iter. 36/49: loss=5.3425548970201335, w0=71.94363327147556, w1=11.053585611823946\n",
      "SGD iter. 37/49: loss=9.740332336448988, w0=72.38500233102661, w1=12.459351789133931\n",
      "SGD iter. 38/49: loss=2.646016029062245, w0=72.61504650196177, w1=12.151910265168457\n",
      "SGD iter. 39/49: loss=0.04363019225585005, w0=72.64458637070076, w1=12.125619217279874\n",
      "SGD iter. 40/49: loss=28.1152862317295, w0=73.39445684260548, w1=11.344732966482704\n",
      "SGD iter. 41/49: loss=0.332047724184607, w0=73.31296479094587, w1=11.373513576623777\n",
      "SGD iter. 42/49: loss=9.877119418208071, w0=73.75742219988868, w1=11.556670945287149\n",
      "SGD iter. 43/49: loss=0.008247501976035807, w0=73.74457891216626, w1=11.585147009380377\n",
      "SGD iter. 44/49: loss=36.81088508682095, w0=74.60261020631376, w1=11.544114864100493\n",
      "SGD iter. 45/49: loss=53.62145037460315, w0=73.56702872352639, w1=11.759480706676825\n",
      "SGD iter. 46/49: loss=20.38976592087829, w0=72.92844018744266, w1=11.67183059809765\n",
      "SGD iter. 47/49: loss=0.3632741682239242, w0=73.01367799222714, w1=11.76157012030891\n",
      "SGD iter. 48/49: loss=11.679672819440981, w0=73.49699306170959, w1=11.56986498200418\n",
      "SGD iter. 49/49: loss=0.011499215729677655, w0=73.48182782796283, w1=11.5563337432553\n",
      "SGD: execution time=0.007 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95c6f11b57b4be29314fa8c1914d413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "# ***************************************************\n",
    "#raise NotImplementedError\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(height, weight, marker=\".\", color=\"b\", s=5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.282124701595, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260339, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260336, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989093\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "# ***************************************************\n",
    "#raise NotImplementedError\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6311b166b18b4fa6ade830a5a684596a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    error=y-np.dot(tx,w)\n",
    "    \n",
    "    #if differentiable, it's the sign\n",
    "    error[error!=0]=np.sign(error[error!=0])\n",
    "    \n",
    "    #if not, set at 0.5 with a random sign\n",
    "    error[error==0]=np.random.randint(0,1)-0.5\n",
    "    \n",
    "    \n",
    "    return np.dot(-tx.T,error)/len(error)\n",
    "    \n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        loss=np.sum(np.abs(y-np.dot(tx,w)))/len(y)\n",
    "        \n",
    "        # ***************************************************\n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        \n",
    "        w=w-gamma*compute_subgradient_mae(y,tx,w)\n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=27.173270209668917, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=26.4904515637512, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=23.899295346035586, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=23.28439292565714, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=22.68687644418184, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=21.53781882800843, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=19.91191015895784, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=19.389644090563227, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=18.887989064395878, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=18.41596050185423, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=17.954898543040382, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=17.505757656579817, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=17.074957426931608, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=16.652967297509893, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=16.248540731496718, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=15.849105212654152, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=15.466919791231321, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=15.108294621512211, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=14.754896345922827, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=14.404528961620272, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=14.05578702812727, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=13.714620911605627, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=13.381236307284146, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=13.058821615166227, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=12.740251724339231, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=12.423218888756102, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=12.107561731901159, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=11.800622097398126, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=11.495041794646415, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=11.189461491894704, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=10.883881189142992, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=10.58459340831319, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=10.295816534318933, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=10.01135208122135, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=9.728084326668117, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=9.448125461122496, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=9.171041104096656, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=8.903656131158947, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=8.63627115822124, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=8.376151920302359, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=8.140540838751482, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=7.918544501597259, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=7.7052797283769845, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=7.493695831178626, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=7.2899924057434, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=7.097234035781528, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=6.919905294668907, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=6.7505735273154395, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=6.584744810805652, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=6.4303432763477915, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=6.27807148189034, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=6.133663329263311, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=6.005840798343018, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=5.885021825223206, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=5.771635252269647, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=5.667162061790248, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=5.586726765993136, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=5.523847812160378, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=5.480093708591866, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=5.453088003502018, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=5.4273926308629, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=5.407322445682747, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=5.387252260502595, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=5.370460780338691, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=5.335714659517469, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=5.330043910465359, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=5.322176726526588, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=5.317240048565146, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=5.315557122666141, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=5.313876880922164, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=5.3123778390243865, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=5.312132229725042, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=5.311886620425695, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=5.311505047641535, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=5.3114827328344205, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=5.311393473605971, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=5.311304214377522, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=5.31128189957041, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=5.311125695920622, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=5.311036436692172, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=5.310859611715928, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=5.310837296908815, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=5.310823570190172, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=5.310727416353907, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=5.310684480220336, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=5.3106135738747895, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=5.310574966149885, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=5.310576651555034, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=5.310574635520698, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=5.310626930749844, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=5.310626494042512, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=5.310623394482, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=5.310577014443433, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=5.310624310505836, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=5.3105760279022824, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=5.310579393366167, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=5.310576945786756, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=5.310579062736981, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=5.310625226529675, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=5.310622563676497, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=5.310576054815084, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=5.310577740220233, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=5.310623916407669, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=5.310626142553513, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=5.310584288862545, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=5.310580527767431, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=5.310577111221071, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=5.310577684556792, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=5.310575095186737, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=5.310580438210212, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=5.310575788704324, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=5.310625791064515, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=5.310584109748109, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=5.31057648222191, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=5.310578167627058, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=5.3105761515927234, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=5.310577836997869, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=5.310626707088355, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=5.310624044235176, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=5.310626270381018, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=5.310625833673685, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=5.310578861144642, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=5.310575159705164, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=5.31057684511031, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=5.310578423326829, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=5.310578530515457, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=5.310576514481121, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=5.310579341211301, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=5.310577869257082, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=5.310579554662228, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=5.310578893403855, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=5.310575191964375, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.025 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55f3282474e4e588580655bf51d0751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        for mini_y, mini_tx in batch_iter(y,tx,batch_size):\n",
    "            loss=np.sum(np.abs(mini_y-np.dot(mini_tx,w)))/len(mini_y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            w=w-gamma*compute_subgradient_mae(mini_y,mini_tx,w)\n",
    "            ws.append(w)\n",
    "            \n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=85.45328715786273, w0=0.7, w1=0.19634459351722802\n",
      "SubSGD iter. 1/499: loss=58.71812036269343, w0=1.4, w1=0.1825893975417374\n",
      "SubSGD iter. 2/499: loss=76.01690599489869, w0=2.0999999999999996, w1=0.3483956571092345\n",
      "SubSGD iter. 3/499: loss=69.30559272664765, w0=2.8, w1=0.35482353798758093\n",
      "SubSGD iter. 4/499: loss=88.09129176824905, w0=3.5, w1=0.9080294540636943\n",
      "SubSGD iter. 5/499: loss=46.64708595389952, w0=4.2, w1=-0.4459274399695333\n",
      "SubSGD iter. 6/499: loss=54.688125910147384, w0=4.9, w1=-0.9889439013346318\n",
      "SubSGD iter. 7/499: loss=56.659364302791246, w0=5.6000000000000005, w1=-1.3456694196904686\n",
      "SubSGD iter. 8/499: loss=90.55672073483288, w0=6.300000000000001, w1=-0.26179402299916665\n",
      "SubSGD iter. 9/499: loss=53.03228993813497, w0=7.000000000000001, w1=-0.606971110724165\n",
      "SubSGD iter. 10/499: loss=40.21673529795844, w0=7.700000000000001, w1=-1.9609280047573927\n",
      "SubSGD iter. 11/499: loss=55.41954144170886, w0=8.4, w1=-2.5339380877720163\n",
      "SubSGD iter. 12/499: loss=73.80622219604828, w0=9.1, w1=-2.038311827303184\n",
      "SubSGD iter. 13/499: loss=73.36468301828822, w0=9.799999999999999, w1=-1.8854180701886023\n",
      "SubSGD iter. 14/499: loss=56.09611488628913, w0=10.499999999999998, w1=-2.238533821755917\n",
      "SubSGD iter. 15/499: loss=49.315145871884496, w0=11.199999999999998, w1=-2.664319969046656\n",
      "SubSGD iter. 16/499: loss=90.58545176560132, w0=11.899999999999997, w1=-1.6518816311802353\n",
      "SubSGD iter. 17/499: loss=60.60488243122004, w0=12.599999999999996, w1=-1.6095550745767446\n",
      "SubSGD iter. 18/499: loss=55.00085884633628, w0=13.299999999999995, w1=-2.1341879992884842\n",
      "SubSGD iter. 19/499: loss=68.59889818205062, w0=13.999999999999995, w1=-1.7904753781050937\n",
      "SubSGD iter. 20/499: loss=57.36252128099554, w0=14.699999999999994, w1=-1.6121232567252792\n",
      "SubSGD iter. 21/499: loss=42.39337827509841, w0=15.399999999999993, w1=-2.023198441267505\n",
      "SubSGD iter. 22/499: loss=96.32339102633495, w0=16.099999999999994, w1=-0.8423937405676092\n",
      "SubSGD iter. 23/499: loss=56.35593552404596, w0=16.799999999999994, w1=-0.8000671839641185\n",
      "SubSGD iter. 24/499: loss=41.53753799028689, w0=17.499999999999993, w1=-1.0799961764005817\n",
      "SubSGD iter. 25/499: loss=46.062852416389475, w0=18.199999999999992, w1=-1.8193289758445257\n",
      "SubSGD iter. 26/499: loss=53.16987285294735, w0=18.89999999999999, w1=-1.6409768544647112\n",
      "SubSGD iter. 27/499: loss=39.101259423439316, w0=19.59999999999999, w1=-1.9209058469011744\n",
      "SubSGD iter. 28/499: loss=31.504421978895948, w0=20.29999999999999, w1=-2.8977033258430933\n",
      "SubSGD iter. 29/499: loss=51.528913215270016, w0=20.99999999999999, w1=-3.279829310393428\n",
      "SubSGD iter. 30/499: loss=61.73585483616409, w0=21.69999999999999, w1=-3.1269355532788468\n",
      "SubSGD iter. 31/499: loss=79.64906856808999, w0=22.399999999999988, w1=-2.034311038953052\n",
      "SubSGD iter. 32/499: loss=44.920449976781164, w0=23.099999999999987, w1=-2.2391445588282752\n",
      "SubSGD iter. 33/499: loss=39.118266033553525, w0=23.799999999999986, w1=-2.609412649357072\n",
      "SubSGD iter. 34/499: loss=51.222745113114115, w0=24.499999999999986, w1=-2.6418416571244316\n",
      "SubSGD iter. 35/499: loss=24.525630659465868, w0=25.199999999999985, w1=-3.4605000269300596\n",
      "SubSGD iter. 36/499: loss=61.6235568273125, w0=25.899999999999984, w1=-2.8559679210759006\n",
      "SubSGD iter. 37/499: loss=36.92650731273205, w0=26.599999999999984, w1=-3.5426625565820373\n",
      "SubSGD iter. 38/499: loss=28.392792367792868, w0=27.299999999999983, w1=-4.396508827007306\n",
      "SubSGD iter. 39/499: loss=28.52351058854158, w0=27.999999999999982, w1=-4.939525288372405\n",
      "SubSGD iter. 40/499: loss=73.99750543693845, w0=28.69999999999998, w1=-3.8816099815448464\n",
      "SubSGD iter. 41/499: loss=91.64781617651303, w0=29.39999999999998, w1=-2.642293680566845\n",
      "SubSGD iter. 42/499: loss=49.739193203887126, w0=30.09999999999998, w1=-2.3537964160094296\n",
      "SubSGD iter. 43/499: loss=39.60163096272838, w0=30.79999999999998, w1=-2.7497726592312617\n",
      "SubSGD iter. 44/499: loss=28.70417670766448, w0=31.49999999999998, w1=-3.175558806522001\n",
      "SubSGD iter. 45/499: loss=47.71910395224007, w0=32.19999999999998, w1=-3.0322282159274843\n",
      "SubSGD iter. 46/499: loss=39.47890592793131, w0=32.899999999999984, w1=-2.8538760945476698\n",
      "SubSGD iter. 47/499: loss=30.080026316305187, w0=33.59999999999999, w1=-3.0043327709987975\n",
      "SubSGD iter. 48/499: loss=52.829602896098436, w0=34.29999999999999, w1=-2.399800665144638\n",
      "SubSGD iter. 49/499: loss=28.460286499478386, w0=34.99999999999999, w1=-2.9728107481592616\n",
      "SubSGD iter. 50/499: loss=32.045826835583185, w0=35.699999999999996, w1=-3.1776442680344847\n",
      "SubSGD iter. 51/499: loss=16.543660638758368, w0=36.4, w1=-3.7248065702683553\n",
      "SubSGD iter. 52/499: loss=51.34239421021611, w0=37.1, w1=-3.195105596206239\n",
      "SubSGD iter. 53/499: loss=21.749832550864582, w0=37.800000000000004, w1=-3.6347940012914233\n",
      "SubSGD iter. 54/499: loss=24.14919765574151, w0=38.50000000000001, w1=-4.188901075288004\n",
      "SubSGD iter. 55/499: loss=43.58282436981292, w0=39.20000000000001, w1=-4.023448283355378\n",
      "SubSGD iter. 56/499: loss=22.07445070069054, w0=39.90000000000001, w1=-4.393716373884175\n",
      "SubSGD iter. 57/499: loss=21.22809534835121, w0=40.600000000000016, w1=-4.966726456898798\n",
      "SubSGD iter. 58/499: loss=47.524360021029885, w0=41.30000000000002, w1=-4.362194351044639\n",
      "SubSGD iter. 59/499: loss=15.149100760731471, w0=42.00000000000002, w1=-4.48208616453297\n",
      "SubSGD iter. 60/499: loss=13.665439173556564, w0=42.700000000000024, w1=-4.767260659082265\n",
      "SubSGD iter. 61/499: loss=26.846044888017282, w0=43.40000000000003, w1=-5.022692540183836\n",
      "SubSGD iter. 62/499: loss=17.450562800898908, w0=44.10000000000003, w1=-5.576799614180416\n",
      "SubSGD iter. 63/499: loss=4.657249343249092, w0=44.80000000000003, w1=-6.614313807993957\n",
      "SubSGD iter. 64/499: loss=35.1232165026269, w0=45.500000000000036, w1=-6.470983217399441\n",
      "SubSGD iter. 65/499: loss=7.368508662283276, w0=46.20000000000004, w1=-6.976000949223764\n",
      "SubSGD iter. 66/499: loss=48.20726141048141, w0=46.90000000000004, w1=-6.628033902833349\n",
      "SubSGD iter. 67/499: loss=32.39142370275213, w0=47.600000000000044, w1=-6.377752786371293\n",
      "SubSGD iter. 68/499: loss=11.213184813026572, w0=48.30000000000005, w1=-6.734478304727129\n",
      "SubSGD iter. 69/499: loss=52.026482814179666, w0=49.00000000000005, w1=-6.016314980695941\n",
      "SubSGD iter. 70/499: loss=12.246148166248275, w0=48.30000000000005, w1=-4.662358086662714\n",
      "SubSGD iter. 71/499: loss=73.43009311027048, w0=49.00000000000005, w1=-3.4230417856847124\n",
      "SubSGD iter. 72/499: loss=47.824462841369574, w0=49.70000000000005, w1=-2.6910771921770773\n",
      "SubSGD iter. 73/499: loss=5.313979921012077, w0=49.00000000000005, w1=-1.5070749917831858\n",
      "SubSGD iter. 74/499: loss=44.03591173684423, w0=49.70000000000005, w1=-1.0671505488990038\n",
      "SubSGD iter. 75/499: loss=7.67023710056116, w0=50.400000000000055, w1=-1.6882021224695973\n",
      "SubSGD iter. 76/499: loss=37.6438247239952, w0=51.10000000000006, w1=-1.4100751536679095\n",
      "SubSGD iter. 77/499: loss=25.934444652705103, w0=51.80000000000006, w1=-1.3947674869557736\n",
      "SubSGD iter. 78/499: loss=31.30517190274029, w0=52.500000000000064, w1=-1.0162277035990421\n",
      "SubSGD iter. 79/499: loss=4.522177916779604, w0=53.20000000000007, w1=-1.1361195170873732\n",
      "SubSGD iter. 80/499: loss=17.19044685400425, w0=53.90000000000007, w1=-1.5320957603092056\n",
      "SubSGD iter. 81/499: loss=38.73277546029546, w0=54.60000000000007, w1=-0.6909055920845625\n",
      "SubSGD iter. 82/499: loss=38.85292140951267, w0=55.300000000000075, w1=0.14243199047501431\n",
      "SubSGD iter. 83/499: loss=37.79617632247114, w0=56.00000000000008, w1=0.8743965839826493\n",
      "SubSGD iter. 84/499: loss=0.5890193593340172, w0=55.300000000000075, w1=1.4215588862165196\n",
      "SubSGD iter. 85/499: loss=37.73534764937547, w0=56.00000000000008, w1=2.616297723320945\n",
      "SubSGD iter. 86/499: loss=24.77900704696534, w0=56.70000000000008, w1=3.064956762105655\n",
      "SubSGD iter. 87/499: loss=16.44024305816105, w0=57.400000000000084, w1=3.030265902764184\n",
      "SubSGD iter. 88/499: loss=0.6830286888363517, w0=56.70000000000008, w1=3.4620584810476043\n",
      "SubSGD iter. 89/499: loss=2.1072816769629057, w0=56.00000000000008, w1=4.350890305214097\n",
      "SubSGD iter. 90/499: loss=16.068067263742257, w0=56.70000000000008, w1=3.8262573805023576\n",
      "SubSGD iter. 91/499: loss=3.5870687380400597, w0=57.400000000000084, w1=3.415182195960132\n",
      "SubSGD iter. 92/499: loss=23.970430318180632, w0=58.10000000000009, w1=4.116116193394884\n",
      "SubSGD iter. 93/499: loss=15.584086352238494, w0=58.80000000000009, w1=3.4969750379805014\n",
      "SubSGD iter. 94/499: loss=18.127136811457248, w0=59.50000000000009, w1=3.5122827046926375\n",
      "SubSGD iter. 95/499: loss=12.646910150525386, w0=60.200000000000095, w1=3.328816925642218\n",
      "SubSGD iter. 96/499: loss=4.7700440651410645, w0=60.9000000000001, w1=2.978479992273827\n",
      "SubSGD iter. 97/499: loss=15.689522915642307, w0=61.6000000000001, w1=3.2076016845605855\n",
      "SubSGD iter. 98/499: loss=2.8018424755762226, w0=60.9000000000001, w1=3.49277617910988\n",
      "SubSGD iter. 99/499: loss=4.853461442863981, w0=60.200000000000095, w1=4.1155625859301725\n",
      "SubSGD iter. 100/499: loss=2.258922813731445, w0=59.50000000000009, w1=4.819067504321035\n",
      "SubSGD iter. 101/499: loss=2.9838462208216967, w0=60.200000000000095, w1=4.471983518701045\n",
      "SubSGD iter. 102/499: loss=12.662664030378444, w0=60.9000000000001, w1=4.3449690734730195\n",
      "SubSGD iter. 103/499: loss=8.219103283868073, w0=60.200000000000095, w1=5.025568251129102\n",
      "SubSGD iter. 104/499: loss=16.653808113562476, w0=60.9000000000001, w1=5.521194511597935\n",
      "SubSGD iter. 105/499: loss=35.77541237584934, w0=61.6000000000001, w1=7.061256849985121\n",
      "SubSGD iter. 106/499: loss=20.316694563181898, w0=62.300000000000104, w1=7.175077176799434\n",
      "SubSGD iter. 107/499: loss=0.12633361034490775, w0=63.00000000000011, w1=5.963654288873253\n",
      "SubSGD iter. 108/499: loss=3.610947984377752, w0=62.300000000000104, w1=6.510816591107123\n",
      "SubSGD iter. 109/499: loss=82.51029451978746, w0=63.00000000000011, w1=3.1376729135174464\n",
      "SubSGD iter. 110/499: loss=4.706442064981665, w0=63.70000000000011, w1=2.7717521827870493\n",
      "SubSGD iter. 111/499: loss=4.118769416230421, w0=63.00000000000011, w1=3.428633150540013\n",
      "SubSGD iter. 112/499: loss=2.7579609643104703, w0=63.70000000000011, w1=3.3081285990609706\n",
      "SubSGD iter. 113/499: loss=7.039763997981069, w0=63.00000000000011, w1=4.209760863473728\n",
      "SubSGD iter. 114/499: loss=2.410940061783009, w0=63.70000000000011, w1=3.8594239301053372\n",
      "SubSGD iter. 115/499: loss=10.384940537091566, w0=63.00000000000011, w1=4.695462787378153\n",
      "SubSGD iter. 116/499: loss=16.673559819030757, w0=63.70000000000011, w1=5.287780748646331\n",
      "SubSGD iter. 117/499: loss=12.507659305894308, w0=64.4000000000001, w1=5.4535870082138285\n",
      "SubSGD iter. 118/499: loss=5.116815718127285, w0=65.10000000000011, w1=5.6319391295936425\n",
      "SubSGD iter. 119/499: loss=2.378542477455319, w0=65.80000000000011, w1=5.431189592544391\n",
      "SubSGD iter. 120/499: loss=22.190758095240597, w0=66.50000000000011, w1=6.523814106870185\n",
      "SubSGD iter. 121/499: loss=2.5470496742281625, w0=67.20000000000012, w1=6.148228268583615\n",
      "SubSGD iter. 122/499: loss=4.558946823524835, w0=67.90000000000012, w1=6.571769521637631\n",
      "SubSGD iter. 123/499: loss=14.994358603451545, w0=68.60000000000012, w1=7.412959689862274\n",
      "SubSGD iter. 124/499: loss=11.533203522405827, w0=69.30000000000013, w1=8.026874103239857\n",
      "SubSGD iter. 125/499: loss=2.619641856299495, w0=70.00000000000013, w1=8.069200659843348\n",
      "SubSGD iter. 126/499: loss=6.385992910723083, w0=69.30000000000013, w1=8.639160784379333\n",
      "SubSGD iter. 127/499: loss=5.578559121318385, w0=68.60000000000012, w1=8.984337872104332\n",
      "SubSGD iter. 128/499: loss=7.515390706373857, w0=69.30000000000013, w1=8.588361628882499\n",
      "SubSGD iter. 129/499: loss=9.689728514322582, w0=70.00000000000013, w1=9.322637372374299\n",
      "SubSGD iter. 130/499: loss=5.783818296705888, w0=70.70000000000013, w1=9.927169478228459\n",
      "SubSGD iter. 131/499: loss=0.9061970650490991, w0=70.00000000000013, w1=10.530935548194972\n",
      "SubSGD iter. 132/499: loss=1.7660040471590719, w0=69.30000000000013, w1=11.742358436121151\n",
      "SubSGD iter. 133/499: loss=3.8923105040390453, w0=70.00000000000013, w1=11.723778977222274\n",
      "SubSGD iter. 134/499: loss=1.3603666218152881, w0=70.70000000000013, w1=12.059007032525528\n",
      "SubSGD iter. 135/499: loss=8.685551524400282, w0=71.40000000000013, w1=12.21190078964011\n",
      "SubSGD iter. 136/499: loss=6.995386674080066, w0=70.70000000000013, w1=12.42915779381986\n",
      "SubSGD iter. 137/499: loss=6.227957237129829, w0=70.00000000000013, w1=12.64641479799961\n",
      "SubSGD iter. 138/499: loss=9.257250124083697, w0=70.70000000000013, w1=12.799308555114191\n",
      "SubSGD iter. 139/499: loss=8.111822088546106, w0=71.40000000000013, w1=13.41569550395144\n",
      "SubSGD iter. 140/499: loss=8.407538376139229, w0=72.10000000000014, w1=13.065757572471123\n",
      "SubSGD iter. 141/499: loss=5.96576157684683, w0=72.80000000000014, w1=13.717814469913577\n",
      "SubSGD iter. 142/499: loss=7.420136460739386, w0=72.10000000000014, w1=13.74311223530242\n",
      "SubSGD iter. 143/499: loss=1.1454146797228333, w0=72.80000000000014, w1=13.724532776403542\n",
      "SubSGD iter. 144/499: loss=6.536609379299975, w0=73.50000000000014, w1=14.44269610043473\n",
      "SubSGD iter. 145/499: loss=1.7422989113743768, w0=72.80000000000014, w1=14.192414983972673\n",
      "SubSGD iter. 146/499: loss=7.226871944845598, w0=72.10000000000014, w1=13.234279893070866\n",
      "SubSGD iter. 147/499: loss=2.599620123097793, w0=71.40000000000013, w1=13.777296354435965\n",
      "SubSGD iter. 148/499: loss=2.310630992526441, w0=72.10000000000014, w1=14.835211661263523\n",
      "SubSGD iter. 149/499: loss=2.1469243506559366, w0=72.80000000000014, w1=14.13862691002276\n",
      "SubSGD iter. 150/499: loss=8.924194025322564, w0=73.50000000000014, w1=14.801771227065807\n",
      "SubSGD iter. 151/499: loss=6.301914962363611, w0=72.80000000000014, w1=15.337290909892012\n",
      "SubSGD iter. 152/499: loss=10.515418176961589, w0=73.50000000000014, w1=15.685257956282426\n",
      "SubSGD iter. 153/499: loss=1.5928233320257519, w0=74.20000000000014, w1=14.988673205041662\n",
      "SubSGD iter. 154/499: loss=0.5866609947275094, w0=74.90000000000015, w1=15.82201078760124\n",
      "SubSGD iter. 155/499: loss=2.4907525910641155, w0=75.60000000000015, w1=14.61058789967506\n",
      "SubSGD iter. 156/499: loss=6.80077501752136, w0=74.90000000000015, w1=15.418701640716993\n",
      "SubSGD iter. 157/499: loss=8.374949246176357, w0=75.60000000000015, w1=15.766668687107407\n",
      "SubSGD iter. 158/499: loss=3.2870245618024043, w0=76.30000000000015, w1=14.412711793074179\n",
      "SubSGD iter. 159/499: loss=5.794876824261074, w0=75.60000000000015, w1=15.069592760827142\n",
      "SubSGD iter. 160/499: loss=10.742444655123279, w0=76.30000000000015, w1=15.41406100769761\n",
      "SubSGD iter. 161/499: loss=6.427303380227556, w0=75.60000000000015, w1=15.809497799640615\n",
      "SubSGD iter. 162/499: loss=12.068165824255864, w0=74.90000000000015, w1=16.080977981623196\n",
      "SubSGD iter. 163/499: loss=3.2336809032709652, w0=74.20000000000014, w1=15.319621031004127\n",
      "SubSGD iter. 164/499: loss=1.4354266057247855, w0=73.50000000000014, w1=14.791565215493478\n",
      "SubSGD iter. 165/499: loss=3.394613670703805, w0=72.80000000000014, w1=14.992314752542729\n",
      "SubSGD iter. 166/499: loss=5.456143807609337, w0=72.10000000000014, w1=15.527834435368934\n",
      "SubSGD iter. 167/499: loss=3.469458868110493, w0=72.80000000000014, w1=16.173981668756156\n",
      "SubSGD iter. 168/499: loss=12.99897763944115, w0=73.50000000000014, w1=16.518449915626622\n",
      "SubSGD iter. 169/499: loss=2.970196779625468, w0=74.20000000000014, w1=17.23661323965781\n",
      "SubSGD iter. 170/499: loss=1.4906922676001813, w0=73.50000000000014, w1=17.667623924406424\n",
      "SubSGD iter. 171/499: loss=4.592915299674928, w0=72.80000000000014, w1=18.107819179113395\n",
      "SubSGD iter. 172/499: loss=6.008160489125849, w0=72.10000000000014, w1=18.58586738153952\n",
      "SubSGD iter. 173/499: loss=0.5651575520646546, w0=71.40000000000013, w1=18.942592899895356\n",
      "SubSGD iter. 174/499: loss=0.10416097652907297, w0=70.70000000000013, w1=19.143342436944607\n",
      "SubSGD iter. 175/499: loss=4.112433821864926, w0=70.00000000000013, w1=18.050717922618812\n",
      "SubSGD iter. 176/499: loss=8.973324476672417, w0=70.70000000000013, w1=17.073920443676894\n",
      "SubSGD iter. 177/499: loss=1.7472065806653632, w0=71.40000000000013, w1=17.184220477137686\n",
      "SubSGD iter. 178/499: loss=0.3642817247044263, w0=70.70000000000013, w1=17.75418060167367\n",
      "SubSGD iter. 179/499: loss=2.554791977461065, w0=71.40000000000013, w1=17.919986861241167\n",
      "SubSGD iter. 180/499: loss=5.223784274771191, w0=70.70000000000013, w1=18.137243865420917\n",
      "SubSGD iter. 181/499: loss=3.6045100148222105, w0=70.00000000000013, w1=17.232068762233855\n",
      "SubSGD iter. 182/499: loss=3.484208863962607, w0=69.30000000000013, w1=17.05371664085404\n",
      "SubSGD iter. 183/499: loss=6.070163773403898, w0=70.00000000000013, w1=16.259347296363263\n",
      "SubSGD iter. 184/499: loss=6.1522007383311745, w0=70.70000000000013, w1=16.87400827749365\n",
      "SubSGD iter. 185/499: loss=1.759115606014447, w0=71.40000000000013, w1=17.99231849810513\n",
      "SubSGD iter. 186/499: loss=0.4873143798750377, w0=72.10000000000014, w1=17.28881357971427\n",
      "SubSGD iter. 187/499: loss=6.760897042991232, w0=72.80000000000014, w1=17.5542972819385\n",
      "SubSGD iter. 188/499: loss=1.319049223486985, w0=73.50000000000014, w1=18.18657171042019\n",
      "SubSGD iter. 189/499: loss=2.4209765533670407, w0=72.80000000000014, w1=18.38732124746944\n",
      "SubSGD iter. 190/499: loss=8.96935736295373, w0=73.50000000000014, w1=17.860132608722214\n",
      "SubSGD iter. 191/499: loss=4.610702019696802, w0=74.20000000000014, w1=18.473125105905773\n",
      "SubSGD iter. 192/499: loss=4.295611098507138, w0=73.50000000000014, w1=17.354814885294292\n",
      "SubSGD iter. 193/499: loss=2.250571897589623, w0=72.80000000000014, w1=17.348387004415947\n",
      "SubSGD iter. 194/499: loss=3.653682286125381, w0=72.10000000000014, w1=17.883906687242153\n",
      "SubSGD iter. 195/499: loss=7.763650249523927, w0=71.40000000000013, w1=18.15538686922473\n",
      "SubSGD iter. 196/499: loss=2.425677146929459, w0=70.70000000000013, w1=17.31419670100009\n",
      "SubSGD iter. 197/499: loss=8.197885452799184, w0=71.40000000000013, w1=16.741186617985466\n",
      "SubSGD iter. 198/499: loss=2.371336560790951, w0=70.70000000000013, w1=15.861456488152989\n",
      "SubSGD iter. 199/499: loss=4.823958360319274, w0=71.40000000000013, w1=15.090633422084801\n",
      "SubSGD iter. 200/499: loss=5.2410818359482505, w0=70.70000000000013, w1=14.891872556266277\n",
      "SubSGD iter. 201/499: loss=3.7562759900516127, w0=71.40000000000013, w1=14.12104949019809\n",
      "SubSGD iter. 202/499: loss=1.673435146065799, w0=70.70000000000013, w1=14.560737895283275\n",
      "SubSGD iter. 203/499: loss=3.040517694820636, w0=71.40000000000013, w1=15.65336240960907\n",
      "SubSGD iter. 204/499: loss=7.043388387785171, w0=70.70000000000013, w1=14.70645014786664\n",
      "SubSGD iter. 205/499: loss=9.949176319297983, w0=71.40000000000013, w1=13.679517950274107\n",
      "SubSGD iter. 206/499: loss=4.154704817555057, w0=70.70000000000013, w1=14.520936460000051\n",
      "SubSGD iter. 207/499: loss=9.628532083538218, w0=71.40000000000013, w1=15.533614801352819\n",
      "SubSGD iter. 208/499: loss=6.021188204490272, w0=72.10000000000014, w1=15.699067593285445\n",
      "SubSGD iter. 209/499: loss=8.412182503520967, w0=72.80000000000014, w1=15.812887920099758\n",
      "SubSGD iter. 210/499: loss=8.604942830314585, w0=73.50000000000014, w1=15.073555120655815\n",
      "SubSGD iter. 211/499: loss=3.2583813575537803, w0=74.20000000000014, w1=14.519448046659233\n",
      "SubSGD iter. 212/499: loss=4.223190537667762, w0=73.50000000000014, w1=14.959136451744419\n",
      "SubSGD iter. 213/499: loss=6.172946617902831, w0=72.80000000000014, w1=15.241684270830644\n",
      "SubSGD iter. 214/499: loss=3.1795852855238707, w0=72.10000000000014, w1=14.746058010361812\n",
      "SubSGD iter. 215/499: loss=8.980598435000026, w0=71.40000000000013, w1=15.017538192344391\n",
      "SubSGD iter. 216/499: loss=6.610053568582259, w0=72.10000000000014, w1=15.73570151637558\n",
      "SubSGD iter. 217/499: loss=6.408162744357519, w0=72.80000000000014, w1=14.88185524595031\n",
      "SubSGD iter. 218/499: loss=7.84506161273579, w0=73.50000000000014, w1=14.995675572764624\n",
      "SubSGD iter. 219/499: loss=3.9353553689160847, w0=72.80000000000014, w1=15.522632608533472\n",
      "SubSGD iter. 220/499: loss=7.013975544671396, w0=73.50000000000014, w1=15.12665636531164\n",
      "SubSGD iter. 221/499: loss=6.547924563721317, w0=72.80000000000014, w1=15.684780069060679\n",
      "SubSGD iter. 222/499: loss=4.9285611740768545, w0=72.10000000000014, w1=16.03186405468067\n",
      "SubSGD iter. 223/499: loss=12.370707246503848, w0=71.40000000000013, w1=16.045619250656163\n",
      "SubSGD iter. 224/499: loss=5.936010376147607, w0=70.70000000000013, w1=16.070917016045005\n",
      "SubSGD iter. 225/499: loss=2.80972076762248, w0=71.40000000000013, w1=16.273353906086005\n",
      "SubSGD iter. 226/499: loss=10.461561340934267, w0=72.10000000000014, w1=16.713278348970185\n",
      "SubSGD iter. 227/499: loss=4.2023684521811475, w0=71.40000000000013, w1=16.895142192545354\n",
      "SubSGD iter. 228/499: loss=6.1435694476443246, w0=70.70000000000013, w1=16.47160093949134\n",
      "SubSGD iter. 229/499: loss=12.644403814872248, w0=71.40000000000013, w1=17.596730899621647\n",
      "SubSGD iter. 230/499: loss=3.6646143556587134, w0=72.10000000000014, w1=16.69509863520889\n",
      "SubSGD iter. 231/499: loss=5.988629481958199, w0=72.80000000000014, w1=16.72147128583371\n",
      "SubSGD iter. 232/499: loss=1.6413685451691435, w0=72.10000000000014, w1=16.432974021276294\n",
      "SubSGD iter. 233/499: loss=5.182067494015385, w0=72.80000000000014, w1=16.177542140174722\n",
      "SubSGD iter. 234/499: loss=0.43049859365972054, w0=73.50000000000014, w1=17.71760447856191\n",
      "SubSGD iter. 235/499: loss=8.60744679261279, w0=74.20000000000014, w1=18.95692077953991\n",
      "SubSGD iter. 236/499: loss=7.513731360735548, w0=74.90000000000015, w1=18.71594086487326\n",
      "SubSGD iter. 237/499: loss=13.916298503771209, w0=74.20000000000014, w1=18.659631249827957\n",
      "SubSGD iter. 238/499: loss=3.4798505911787316, w0=74.90000000000015, w1=19.200588301799097\n",
      "SubSGD iter. 239/499: loss=7.36188003945918, w0=74.20000000000014, w1=19.67863650422522\n",
      "SubSGD iter. 240/499: loss=9.097696073224313, w0=73.50000000000014, w1=18.77346140103816\n",
      "SubSGD iter. 241/499: loss=1.889296465967547, w0=72.80000000000014, w1=18.264275557091906\n",
      "SubSGD iter. 242/499: loss=2.7445472749986024, w0=73.50000000000014, w1=17.480042829052582\n",
      "SubSGD iter. 243/499: loss=0.8501556468070532, w0=72.80000000000014, w1=16.46760449118616\n",
      "SubSGD iter. 244/499: loss=0.22601953363124494, w0=73.50000000000014, w1=17.300942073745738\n",
      "SubSGD iter. 245/499: loss=1.6541450963311917, w0=74.20000000000014, w1=18.03290666725337\n",
      "SubSGD iter. 246/499: loss=5.879163331080832, w0=73.50000000000014, w1=16.940282152927576\n",
      "SubSGD iter. 247/499: loss=5.416218345315357, w0=74.20000000000014, w1=17.553274650111135\n",
      "SubSGD iter. 248/499: loss=1.0421099526569861, w0=74.90000000000015, w1=17.187353919380737\n",
      "SubSGD iter. 249/499: loss=6.657192388407566, w0=74.20000000000014, w1=16.691727658911905\n",
      "SubSGD iter. 250/499: loss=1.0737925339968086, w0=73.50000000000014, w1=17.312779232482498\n",
      "SubSGD iter. 251/499: loss=5.396725133152124, w0=74.20000000000014, w1=16.739769149467875\n",
      "SubSGD iter. 252/499: loss=7.558776463546522, w0=73.50000000000014, w1=16.561417028088062\n",
      "SubSGD iter. 253/499: loss=5.403961799289604, w0=72.80000000000014, w1=17.119540731837102\n",
      "SubSGD iter. 254/499: loss=0.3599818754925934, w0=72.10000000000014, w1=17.00924069837631\n",
      "SubSGD iter. 255/499: loss=2.292368187997681, w0=72.80000000000014, w1=17.661297595818766\n",
      "SubSGD iter. 256/499: loss=2.48872345328833, w0=73.50000000000014, w1=18.12787972178534\n",
      "SubSGD iter. 257/499: loss=0.6197339127081278, w0=74.20000000000014, w1=18.74426667062259\n",
      "SubSGD iter. 258/499: loss=3.7253594338929332, w0=74.90000000000015, w1=18.897160427737173\n",
      "SubSGD iter. 259/499: loss=6.540379715746397, w0=75.60000000000015, w1=19.14147315144631\n",
      "SubSGD iter. 260/499: loss=12.640654681442072, w0=74.90000000000015, w1=17.778721756054562\n",
      "SubSGD iter. 261/499: loss=1.8859241301761784, w0=74.20000000000014, w1=17.888895872935304\n",
      "SubSGD iter. 262/499: loss=8.669394976808817, w0=73.50000000000014, w1=17.914193638324146\n",
      "SubSGD iter. 263/499: loss=8.259395149342922, w0=74.20000000000014, w1=19.153509939302147\n",
      "SubSGD iter. 264/499: loss=0.3170917851232744, w0=74.90000000000015, w1=18.264678115135652\n",
      "SubSGD iter. 265/499: loss=1.1608525414810629, w0=75.60000000000015, w1=17.911562363568336\n",
      "SubSGD iter. 266/499: loss=6.389985987369926, w0=74.90000000000015, w1=17.177286620076536\n",
      "SubSGD iter. 267/499: loss=0.7113389819354126, w0=75.60000000000015, w1=17.64386874604311\n",
      "SubSGD iter. 268/499: loss=3.3805634591892613, w0=74.90000000000015, w1=18.48389631750292\n",
      "SubSGD iter. 269/499: loss=7.760410519736027, w0=75.60000000000015, w1=17.959263392791183\n",
      "SubSGD iter. 270/499: loss=1.4354991116040736, w0=74.90000000000015, w1=18.059540492031392\n",
      "SubSGD iter. 271/499: loss=1.357081818345506, w0=74.20000000000014, w1=17.609360745825096\n",
      "SubSGD iter. 272/499: loss=8.251204013918262, w0=74.90000000000015, w1=16.25540385179187\n",
      "SubSGD iter. 273/499: loss=2.030722985988888, w0=74.20000000000014, w1=15.911691230608477\n",
      "SubSGD iter. 274/499: loss=2.529427558797309, w0=73.50000000000014, w1=15.070501062383835\n",
      "SubSGD iter. 275/499: loss=8.172771828563953, w0=74.20000000000014, w1=15.281665456950039\n",
      "SubSGD iter. 276/499: loss=2.001845614308465, w0=73.50000000000014, w1=14.662532061005372\n",
      "SubSGD iter. 277/499: loss=10.150837953637122, w0=74.20000000000014, w1=15.010499107395786\n",
      "SubSGD iter. 278/499: loss=4.943536608112261, w0=73.50000000000014, w1=15.899330931562279\n",
      "SubSGD iter. 279/499: loss=1.3055971606307963, w0=74.20000000000014, w1=15.875365091759187\n",
      "SubSGD iter. 280/499: loss=1.4854964724587632, w0=73.50000000000014, w1=15.985539208639928\n",
      "SubSGD iter. 281/499: loss=5.1098628980000385, w0=72.80000000000014, w1=16.537510640577608\n",
      "SubSGD iter. 282/499: loss=2.7201378348377574, w0=73.50000000000014, w1=17.153897589414857\n",
      "SubSGD iter. 283/499: loss=2.1269288442931753, w0=74.20000000000014, w1=17.620479715381432\n",
      "SubSGD iter. 284/499: loss=0.3747157197720554, w0=73.50000000000014, w1=17.170299969175137\n",
      "SubSGD iter. 285/499: loss=0.7423398902527794, w0=74.20000000000014, w1=17.822356866617593\n",
      "SubSGD iter. 286/499: loss=4.825474912762388, w0=73.50000000000014, w1=16.98116669839295\n",
      "SubSGD iter. 287/499: loss=8.295619343358695, w0=72.80000000000014, w1=16.557625445338935\n",
      "SubSGD iter. 288/499: loss=2.2812927336546878, w0=72.10000000000014, w1=16.95306223728194\n",
      "SubSGD iter. 289/499: loss=0.8348770833413965, w0=72.80000000000014, w1=17.462248081228193\n",
      "SubSGD iter. 290/499: loss=2.121922969900382, w0=72.10000000000014, w1=17.211966964766138\n",
      "SubSGD iter. 291/499: loss=1.5407614436826407, w0=71.40000000000013, w1=17.396127743084456\n",
      "SubSGD iter. 292/499: loss=4.067476147219658, w0=70.70000000000013, w1=17.546584419535584\n",
      "SubSGD iter. 293/499: loss=1.488680884408069, w0=71.40000000000013, w1=16.738470678493652\n",
      "SubSGD iter. 294/499: loss=0.7272240257716334, w0=70.70000000000013, w1=17.308430803029637\n",
      "SubSGD iter. 295/499: loss=2.2805098275396887, w0=71.40000000000013, w1=17.103597283154414\n",
      "SubSGD iter. 296/499: loss=0.8688813967967661, w0=72.10000000000014, w1=17.93693486571399\n",
      "SubSGD iter. 297/499: loss=1.5916445961730687, w0=71.40000000000013, w1=17.686653749251935\n",
      "SubSGD iter. 298/499: loss=3.9118330632595146, w0=72.10000000000014, w1=17.320733018521537\n",
      "SubSGD iter. 299/499: loss=4.042873009872963, w0=71.40000000000013, w1=17.706017158849665\n",
      "SubSGD iter. 300/499: loss=127.3574659689719, w0=72.10000000000014, w1=14.332873481259988\n",
      "SubSGD iter. 301/499: loss=3.66670518586929, w0=72.80000000000014, w1=14.603011931549657\n",
      "SubSGD iter. 302/499: loss=0.5051585405820447, w0=73.50000000000014, w1=15.131067747060307\n",
      "SubSGD iter. 303/499: loss=0.23821985670046786, w0=74.20000000000014, w1=15.096376887718836\n",
      "SubSGD iter. 304/499: loss=6.053772461482829, w0=73.50000000000014, w1=15.721192987529783\n",
      "SubSGD iter. 305/499: loss=3.522336682040674, w0=74.20000000000014, w1=15.465761106428213\n",
      "SubSGD iter. 306/499: loss=3.615934802443512, w0=73.50000000000014, w1=15.891547253718953\n",
      "SubSGD iter. 307/499: loss=7.2175081322978585, w0=74.20000000000014, w1=15.366914329007214\n",
      "SubSGD iter. 308/499: loss=3.9296121447329853, w0=73.50000000000014, w1=15.567663866056465\n",
      "SubSGD iter. 309/499: loss=6.201306913193704, w0=72.80000000000014, w1=16.403702723329282\n",
      "SubSGD iter. 310/499: loss=8.394582990059675, w0=72.10000000000014, w1=17.084301900985366\n",
      "SubSGD iter. 311/499: loss=6.833611696590154, w0=72.80000000000014, w1=17.747446218028415\n",
      "SubSGD iter. 312/499: loss=7.188372668060339, w0=72.10000000000014, w1=16.691044986649526\n",
      "SubSGD iter. 313/499: loss=3.284942738956346, w0=72.80000000000014, w1=17.307431935486775\n",
      "SubSGD iter. 314/499: loss=1.4371503163025636, w0=72.10000000000014, w1=16.858772896702064\n",
      "SubSGD iter. 315/499: loss=3.328250845654196, w0=71.40000000000013, w1=17.39429257952827\n",
      "SubSGD iter. 316/499: loss=9.621481031969623, w0=72.10000000000014, w1=16.867103940781043\n",
      "SubSGD iter. 317/499: loss=3.0162153953589126, w0=72.80000000000014, w1=17.746351906677216\n",
      "SubSGD iter. 318/499: loss=2.258189750743412, w0=73.50000000000014, w1=18.173868991894633\n",
      "SubSGD iter. 319/499: loss=13.191057092021666, w0=72.80000000000014, w1=18.293760805382963\n",
      "SubSGD iter. 320/499: loss=10.974987126654163, w0=73.50000000000014, w1=16.939803911349735\n",
      "SubSGD iter. 321/499: loss=0.5554786996884502, w0=72.80000000000014, w1=16.430618067403483\n",
      "SubSGD iter. 322/499: loss=2.353040433774531, w0=72.10000000000014, w1=16.82605485934649\n",
      "SubSGD iter. 323/499: loss=1.7513262306150068, w0=71.40000000000013, w1=16.09177911585469\n",
      "SubSGD iter. 324/499: loss=6.79072356354326, w0=72.10000000000014, w1=15.507466578308797\n",
      "SubSGD iter. 325/499: loss=4.7240130886591345, w0=72.80000000000014, w1=16.38671454420497\n",
      "SubSGD iter. 326/499: loss=3.102781622127935, w0=73.50000000000014, w1=16.35428553643761\n",
      "SubSGD iter. 327/499: loss=4.7497731215836865, w0=72.80000000000014, w1=15.27041013974631\n",
      "SubSGD iter. 328/499: loss=5.784400325757389, w0=72.10000000000014, w1=15.092058018366496\n",
      "SubSGD iter. 329/499: loss=3.3306292431372384, w0=72.80000000000014, w1=16.45742747103011\n",
      "SubSGD iter. 330/499: loss=4.177542902552489, w0=72.10000000000014, w1=16.53824177651882\n",
      "SubSGD iter. 331/499: loss=0.8630975528417366, w0=72.80000000000014, w1=15.4484467534456\n",
      "SubSGD iter. 332/499: loss=5.045743329960381, w0=72.10000000000014, w1=15.79553073906559\n",
      "SubSGD iter. 333/499: loss=11.91438094705289, w0=72.80000000000014, w1=15.176389583651208\n",
      "SubSGD iter. 334/499: loss=2.071583348196512, w0=72.10000000000014, w1=14.571857477797048\n",
      "SubSGD iter. 335/499: loss=4.050051796247288, w0=71.40000000000013, w1=15.07687520962137\n",
      "SubSGD iter. 336/499: loss=2.4750444422483397, w0=70.70000000000013, w1=15.918293719347314\n",
      "SubSGD iter. 337/499: loss=0.15546120916730644, w0=71.40000000000013, w1=15.47860531426213\n",
      "SubSGD iter. 338/499: loss=3.4871789331186775, w0=72.10000000000014, w1=14.782020563021366\n",
      "SubSGD iter. 339/499: loss=5.24488142264736, w0=72.80000000000014, w1=13.928174292596097\n",
      "SubSGD iter. 340/499: loss=2.530964333102368, w0=73.50000000000014, w1=12.574217398562869\n",
      "SubSGD iter. 341/499: loss=8.161465675611382, w0=72.80000000000014, w1=12.599515163951711\n",
      "SubSGD iter. 342/499: loss=0.15384996532155526, w0=73.50000000000014, w1=13.204047269805871\n",
      "SubSGD iter. 343/499: loss=7.236625217638235, w0=74.20000000000014, w1=13.757253185881984\n",
      "SubSGD iter. 344/499: loss=6.627922432754403, w0=74.90000000000015, w1=13.871073512696297\n",
      "SubSGD iter. 345/499: loss=1.1793935899998402, w0=75.60000000000015, w1=13.298063429681674\n",
      "SubSGD iter. 346/499: loss=1.28838229443123, w0=74.90000000000015, w1=13.095626539640675\n",
      "SubSGD iter. 347/499: loss=0.15463805709794087, w0=75.60000000000015, w1=13.936816707865317\n",
      "SubSGD iter. 348/499: loss=1.4731069046297876, w0=74.90000000000015, w1=13.734379817824317\n",
      "SubSGD iter. 349/499: loss=2.889044354202966, w0=74.20000000000014, w1=13.48409870136226\n",
      "SubSGD iter. 350/499: loss=0.5604629939202255, w0=74.90000000000015, w1=13.300632922311841\n",
      "SubSGD iter. 351/499: loss=3.470379521296479, w0=75.60000000000015, w1=13.91701987114909\n",
      "SubSGD iter. 352/499: loss=1.0235416089577285, w0=74.90000000000015, w1=14.044034316377116\n",
      "SubSGD iter. 353/499: loss=10.626327185977381, w0=74.20000000000014, w1=14.32396330881358\n",
      "SubSGD iter. 354/499: loss=0.12788895270682588, w0=73.50000000000014, w1=13.710048895435998\n",
      "SubSGD iter. 355/499: loss=6.3826239544066965, w0=72.80000000000014, w1=13.891912739011167\n",
      "SubSGD iter. 356/499: loss=3.136877259511678, w0=73.50000000000014, w1=14.162051189300836\n",
      "SubSGD iter. 357/499: loss=6.4795203092257765, w0=74.20000000000014, w1=14.71525710537695\n",
      "SubSGD iter. 358/499: loss=4.120178126022665, w0=74.90000000000015, w1=15.433420429408137\n",
      "SubSGD iter. 359/499: loss=7.925933512854442, w0=74.20000000000014, w1=15.255068308028322\n",
      "SubSGD iter. 360/499: loss=3.4838490892651492, w0=73.50000000000014, w1=14.049052018257875\n",
      "SubSGD iter. 361/499: loss=4.6479695721615855, w0=72.80000000000014, w1=14.576009054026724\n",
      "SubSGD iter. 362/499: loss=7.2985640951334005, w0=73.50000000000014, w1=13.83667625458278\n",
      "SubSGD iter. 363/499: loss=6.396943724489418, w0=72.80000000000014, w1=14.326359787979369\n",
      "SubSGD iter. 364/499: loss=1.275802004654878, w0=72.10000000000014, w1=13.734041826711191\n",
      "SubSGD iter. 365/499: loss=0.28986749342384144, w0=72.80000000000014, w1=14.022539091268607\n",
      "SubSGD iter. 366/499: loss=0.35465635843758037, w0=72.10000000000014, w1=13.14280896143613\n",
      "SubSGD iter. 367/499: loss=6.557395832944302, w0=72.80000000000014, w1=12.61817603672439\n",
      "SubSGD iter. 368/499: loss=5.926915274180814, w0=73.50000000000014, w1=12.655463348342261\n",
      "SubSGD iter. 369/499: loss=5.807123147179837, w0=74.20000000000014, w1=13.5116854008468\n",
      "SubSGD iter. 370/499: loss=4.313189951404979, w0=74.90000000000015, w1=14.87705485351041\n",
      "SubSGD iter. 371/499: loss=2.5068927615424457, w0=74.20000000000014, w1=14.142779110018612\n",
      "SubSGD iter. 372/499: loss=2.650165571487662, w0=73.50000000000014, w1=14.10045255341512\n",
      "SubSGD iter. 373/499: loss=1.225804421101813, w0=74.20000000000014, w1=14.478992336771853\n",
      "SubSGD iter. 374/499: loss=14.501221610510477, w0=73.50000000000014, w1=14.492747532747343\n",
      "SubSGD iter. 375/499: loss=3.0882871354819628, w0=74.20000000000014, w1=13.91973744973272\n",
      "SubSGD iter. 376/499: loss=2.5983238037720184, w0=73.50000000000014, w1=14.71956818379543\n",
      "SubSGD iter. 377/499: loss=1.2652057248898814, w0=74.20000000000014, w1=14.695602343992338\n",
      "SubSGD iter. 378/499: loss=9.315616897607157, w0=73.50000000000014, w1=13.73746725309053\n",
      "SubSGD iter. 379/499: loss=3.203196751747015, w0=74.20000000000014, w1=13.752774919802667\n",
      "SubSGD iter. 380/499: loss=5.889792356146373, w0=73.50000000000014, w1=13.833589225291378\n",
      "SubSGD iter. 381/499: loss=2.2845025924163593, w0=74.20000000000014, w1=13.801160217524018\n",
      "SubSGD iter. 382/499: loss=2.293781634028477, w0=73.50000000000014, w1=12.606421380419592\n",
      "SubSGD iter. 383/499: loss=2.932963893041233, w0=74.20000000000014, w1=12.876559830709262\n",
      "SubSGD iter. 384/499: loss=13.221630667647432, w0=74.90000000000015, w1=13.22102807757973\n",
      "SubSGD iter. 385/499: loss=5.3907248889470765, w0=74.20000000000014, w1=13.40518885589805\n",
      "SubSGD iter. 386/499: loss=3.800853052622955, w0=74.90000000000015, w1=14.523499076509532\n",
      "SubSGD iter. 387/499: loss=4.290913884321441, w0=74.20000000000014, w1=15.180380044262495\n",
      "SubSGD iter. 388/499: loss=8.503296248372763, w0=74.90000000000015, w1=16.36118474496239\n",
      "SubSGD iter. 389/499: loss=9.424605944947942, w0=74.20000000000014, w1=16.386482510351232\n",
      "SubSGD iter. 390/499: loss=6.66383557658915, w0=73.50000000000014, w1=16.933644812585104\n",
      "SubSGD iter. 391/499: loss=0.8110538556720073, w0=74.20000000000014, w1=15.990138329047983\n",
      "SubSGD iter. 392/499: loss=6.522969676385081, w0=74.90000000000015, w1=14.952624135234442\n",
      "SubSGD iter. 393/499: loss=3.254277583687454, w0=75.60000000000015, w1=14.978996785859263\n",
      "SubSGD iter. 394/499: loss=4.0340502460992695, w0=74.90000000000015, w1=14.728715669397205\n",
      "SubSGD iter. 395/499: loss=8.121201936729854, w0=74.20000000000014, w1=14.972742494106356\n",
      "SubSGD iter. 396/499: loss=2.4761836350509725, w0=74.90000000000015, w1=14.988050160818492\n",
      "SubSGD iter. 397/499: loss=0.25463960272472264, w0=75.60000000000015, w1=14.804584381768073\n",
      "SubSGD iter. 398/499: loss=7.980224344367542, w0=76.30000000000015, w1=15.152551428158487\n",
      "SubSGD iter. 399/499: loss=9.254371252564866, w0=75.60000000000015, w1=14.974199306778672\n",
      "SubSGD iter. 400/499: loss=5.972198800622749, w0=74.90000000000015, w1=15.792857676584301\n",
      "SubSGD iter. 401/499: loss=2.7440132015127574, w0=75.60000000000015, w1=16.31703822803602\n",
      "SubSGD iter. 402/499: loss=5.2899062650922986, w0=76.30000000000015, w1=16.595165196837705\n",
      "SubSGD iter. 403/499: loss=4.902864689042445, w0=75.60000000000015, w1=16.462841670483865\n",
      "SubSGD iter. 404/499: loss=7.7674332763550495, w0=74.90000000000015, w1=16.644705514059034\n",
      "SubSGD iter. 405/499: loss=0.9981455542700672, w0=74.20000000000014, w1=15.849535788941687\n",
      "SubSGD iter. 406/499: loss=0.3933066291398646, w0=73.50000000000014, w1=16.649366523004396\n",
      "SubSGD iter. 407/499: loss=5.81239709997358, w0=72.80000000000014, w1=17.034650663332524\n",
      "SubSGD iter. 408/499: loss=1.9499937691685716, w0=73.50000000000014, w1=17.12317817682165\n",
      "SubSGD iter. 409/499: loss=5.940321429596935, w0=72.80000000000014, w1=16.03930278013035\n",
      "SubSGD iter. 410/499: loss=0.7684815932386257, w0=72.10000000000014, w1=16.696183747883314\n",
      "SubSGD iter. 411/499: loss=1.503019804147236, w0=71.40000000000013, w1=17.09162053982632\n",
      "SubSGD iter. 412/499: loss=11.649882734876023, w0=70.70000000000013, w1=17.10537573580181\n",
      "SubSGD iter. 413/499: loss=5.009577868607167, w0=71.40000000000013, w1=16.921909956751392\n",
      "SubSGD iter. 414/499: loss=0.6836626413283255, w0=70.70000000000013, w1=17.122659493800644\n",
      "SubSGD iter. 415/499: loss=0.5515599126986501, w0=71.40000000000013, w1=17.12908737467899\n",
      "SubSGD iter. 416/499: loss=1.511828878018946, w0=72.10000000000014, w1=17.018913257798246\n",
      "SubSGD iter. 417/499: loss=1.232361266091928, w0=72.80000000000014, w1=17.00033379889937\n",
      "SubSGD iter. 418/499: loss=0.4049269460381666, w0=72.10000000000014, w1=17.370601889428166\n",
      "SubSGD iter. 419/499: loss=1.0338576764781635, w0=72.80000000000014, w1=17.573038779469165\n",
      "SubSGD iter. 420/499: loss=3.588540734219059, w0=73.50000000000014, w1=16.96927270950265\n",
      "SubSGD iter. 421/499: loss=3.5818544771137653, w0=74.20000000000014, w1=17.134725501435277\n",
      "SubSGD iter. 422/499: loss=121.80453905144749, w0=74.90000000000015, w1=13.7615818238456\n",
      "SubSGD iter. 423/499: loss=2.5175944303947375, w0=75.60000000000015, w1=14.413638721288054\n",
      "SubSGD iter. 424/499: loss=8.36610461144366, w0=74.90000000000015, w1=14.235286599908239\n",
      "SubSGD iter. 425/499: loss=0.15137458205158794, w0=74.20000000000014, w1=14.25925243971133\n",
      "SubSGD iter. 426/499: loss=3.7594769943162305, w0=73.50000000000014, w1=14.609589373079721\n",
      "SubSGD iter. 427/499: loss=0.3324975890185513, w0=74.20000000000014, w1=14.005823303113207\n",
      "SubSGD iter. 428/499: loss=0.04765103753000943, w0=74.90000000000015, w1=12.794400415187027\n",
      "SubSGD iter. 429/499: loss=7.972428202183771, w0=74.20000000000014, w1=13.920332828688199\n",
      "SubSGD iter. 430/499: loss=0.377059524562128, w0=74.90000000000015, w1=13.793318383460173\n",
      "SubSGD iter. 431/499: loss=1.665428774122418, w0=74.20000000000014, w1=12.587302093689726\n",
      "SubSGD iter. 432/499: loss=5.8444273345853475, w0=74.90000000000015, w1=13.111482645141443\n",
      "SubSGD iter. 433/499: loss=1.438682446176486, w0=74.20000000000014, w1=13.464598396708757\n",
      "SubSGD iter. 434/499: loss=7.752700741853502, w0=74.90000000000015, w1=12.845457241294374\n",
      "SubSGD iter. 435/499: loss=8.4363947219668, w0=74.20000000000014, w1=13.192541226914365\n",
      "SubSGD iter. 436/499: loss=4.935689531523408, w0=74.90000000000015, w1=14.557910679577976\n",
      "SubSGD iter. 437/499: loss=4.285380444046453, w0=74.20000000000014, w1=13.36317184247355\n",
      "SubSGD iter. 438/499: loss=4.900696299129436, w0=74.90000000000015, w1=13.516065599588131\n",
      "SubSGD iter. 439/499: loss=3.31227039309708, w0=74.20000000000014, w1=13.47373904298464\n",
      "SubSGD iter. 440/499: loss=6.894942542303767, w0=73.50000000000014, w1=12.52682678124221\n",
      "SubSGD iter. 441/499: loss=1.9186798616492382, w0=72.80000000000014, w1=12.731660301117433\n",
      "SubSGD iter. 442/499: loss=6.437114398349088, w0=72.10000000000014, w1=13.620492125283926\n",
      "SubSGD iter. 443/499: loss=7.343349636217738, w0=72.80000000000014, w1=14.338655449315114\n",
      "SubSGD iter. 444/499: loss=9.46713871676927, w0=72.10000000000014, w1=14.623829943864408\n",
      "SubSGD iter. 445/499: loss=8.683334892910509, w0=71.40000000000013, w1=14.018426792420422\n",
      "SubSGD iter. 446/499: loss=4.523161256626047, w0=72.10000000000014, w1=13.464319718423841\n",
      "SubSGD iter. 447/499: loss=6.505578031240518, w0=72.80000000000014, w1=14.829689171087452\n",
      "SubSGD iter. 448/499: loss=8.493673883171155, w0=73.50000000000014, w1=15.02603376460468\n",
      "SubSGD iter. 449/499: loss=2.4615087576793115, w0=72.80000000000014, w1=14.761248655413912\n",
      "SubSGD iter. 450/499: loss=10.083888101627792, w0=73.50000000000014, w1=15.005561379123046\n",
      "SubSGD iter. 451/499: loss=6.1274332845729305, w0=74.20000000000014, w1=16.018239720475812\n",
      "SubSGD iter. 452/499: loss=4.312585342370596, w0=74.90000000000015, w1=16.571445636551925\n",
      "SubSGD iter. 453/499: loss=6.327075385287053, w0=75.60000000000015, w1=16.046812711840186\n",
      "SubSGD iter. 454/499: loss=109.71256796037412, w0=76.30000000000015, w1=13.276513014951197\n",
      "SubSGD iter. 455/499: loss=3.06234449650384, w0=77.00000000000016, w1=14.008477608458833\n",
      "SubSGD iter. 456/499: loss=3.6109110813035414, w0=76.30000000000015, w1=14.37439833918923\n",
      "SubSGD iter. 457/499: loss=11.60139071927668, w0=75.60000000000015, w1=13.82600383543142\n",
      "SubSGD iter. 458/499: loss=0.390474528922212, w0=74.90000000000015, w1=13.316817991485168\n",
      "SubSGD iter. 459/499: loss=2.521559520777089, w0=74.20000000000014, w1=12.72450003021699\n",
      "SubSGD iter. 460/499: loss=1.8259648679548093, w0=73.50000000000014, w1=13.10008586850356\n",
      "SubSGD iter. 461/499: loss=5.816026277393675, w0=74.20000000000014, w1=14.465455321167171\n",
      "SubSGD iter. 462/499: loss=2.758741349642385, w0=74.90000000000015, w1=13.611609050741903\n",
      "SubSGD iter. 463/499: loss=1.5034448207726285, w0=75.60000000000015, w1=14.061788796948196\n",
      "SubSGD iter. 464/499: loss=2.859232898205846, w0=76.30000000000015, w1=14.704718495711992\n",
      "SubSGD iter. 465/499: loss=1.568720615987985, w0=77.00000000000016, w1=15.347648194475788\n",
      "SubSGD iter. 466/499: loss=6.44335316235194, w0=76.30000000000015, w1=14.14163190470534\n",
      "SubSGD iter. 467/499: loss=13.047404559266994, w0=75.60000000000015, w1=14.426806399254634\n",
      "SubSGD iter. 468/499: loss=6.046744188676641, w0=74.90000000000015, w1=15.552738812755805\n",
      "SubSGD iter. 469/499: loss=113.4812984843758, w0=75.60000000000015, w1=12.179595135166128\n",
      "SubSGD iter. 470/499: loss=0.35276608927935627, w0=76.30000000000015, w1=12.79350954854371\n",
      "SubSGD iter. 471/499: loss=2.908927589891519, w0=77.00000000000016, w1=13.416997670829888\n",
      "SubSGD iter. 472/499: loss=0.8094752711307223, w0=76.30000000000015, w1=12.53782535859908\n",
      "SubSGD iter. 473/499: loss=15.653315783423459, w0=75.60000000000015, w1=13.218424536255164\n",
      "SubSGD iter. 474/499: loss=3.230456950149417, w0=76.30000000000015, w1=13.841912658541341\n",
      "SubSGD iter. 475/499: loss=3.0232190335106495, w0=77.00000000000016, w1=13.102579859097396\n",
      "SubSGD iter. 476/499: loss=1.9811462580118473, w0=77.70000000000016, w1=14.115018196963817\n",
      "SubSGD iter. 477/499: loss=1.0553272032424843, w0=77.00000000000016, w1=14.451381154639893\n",
      "SubSGD iter. 478/499: loss=12.70523709610027, w0=76.30000000000015, w1=14.929429357066017\n",
      "SubSGD iter. 479/499: loss=8.865822215074573, w0=75.60000000000015, w1=15.111293200641185\n",
      "SubSGD iter. 480/499: loss=5.9483189082468755, w0=74.90000000000015, w1=15.638250236410034\n",
      "SubSGD iter. 481/499: loss=9.79469333744975, w0=74.20000000000014, w1=16.116298438836157\n",
      "SubSGD iter. 482/499: loss=4.28330489039071, w0=73.50000000000014, w1=15.511766332981997\n",
      "SubSGD iter. 483/499: loss=1.0452288973669113, w0=72.80000000000014, w1=15.71659985285722\n",
      "SubSGD iter. 484/499: loss=2.4327084749432544, w0=73.50000000000014, w1=15.98673830314689\n",
      "SubSGD iter. 485/499: loss=1.3096711419591855, w0=74.20000000000014, w1=16.99917664101331\n",
      "SubSGD iter. 486/499: loss=9.006516420022777, w0=73.50000000000014, w1=16.575635387959295\n",
      "SubSGD iter. 487/499: loss=2.371696409684091, w0=72.80000000000014, w1=15.814278437340226\n",
      "SubSGD iter. 488/499: loss=4.661198797206637, w0=73.50000000000014, w1=15.47791547966415\n",
      "SubSGD iter. 489/499: loss=2.256061640450227, w0=72.80000000000014, w1=16.60384789316532\n",
      "SubSGD iter. 490/499: loss=4.144048231644462, w0=72.10000000000014, w1=16.108221632696488\n",
      "SubSGD iter. 491/499: loss=0.5085112720474712, w0=72.80000000000014, w1=15.487170059125894\n",
      "SubSGD iter. 492/499: loss=2.0957018912192, w0=73.50000000000014, w1=14.79058530788513\n",
      "SubSGD iter. 493/499: loss=2.9590675457371844, w0=74.20000000000014, w1=15.442642205327585\n",
      "SubSGD iter. 494/499: loss=0.5899615479839042, w0=74.90000000000015, w1=15.418676365524494\n",
      "SubSGD iter. 495/499: loss=3.669999061832627, w0=74.20000000000014, w1=14.360761058696935\n",
      "SubSGD iter. 496/499: loss=0.42521144215103845, w0=73.50000000000014, w1=15.057345809937699\n",
      "SubSGD iter. 497/499: loss=13.317306696675672, w0=74.20000000000014, w1=16.2966621109157\n",
      "SubSGD iter. 498/499: loss=6.597918636512304, w0=74.90000000000015, w1=16.05568219624905\n",
      "SubSGD iter. 499/499: loss=1.2894018082194805, w0=75.60000000000015, w1=15.078884717307133\n",
      "SubSGD: execution time=0.033 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb0fc5429634f3a85626e54cbb3bc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
